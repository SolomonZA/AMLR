{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing in Tensorflow\n",
    "---\n",
    "\n",
    "Tensorflow is an open-source library widely used for machine learning. It is second only to the Python machine learning library Scikit-learn in terms of use and popularity. Similar to Keras, it allows a user to implement a powerful deep learning model in relatively few lines of code.\n",
    "\n",
    "\n",
    "Many sophisticated applications today make use of Tensorflow-based machine learning to exploit the wealth of data at their disposal. This includes voice recognition, machine translation and sentimement analysis, image recognition, video detection and recommender systems.\n",
    "\n",
    "In this example, we'll implement a simple Recurrent Neural Network which makes use of a Gated Recurrent Unit (GRU) which enables RNNs to have longer term memory, giving them similar performance as LSTMs and sometimes better on small datasets.\n",
    "\n",
    "The model is a character-level RNN which aims to predict the next character in a sequence, based on a prior sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import my_txtutils as txt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tfc\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "# Reset the Tensorflow Computational Graph\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine all the texts to form a data superset\n",
    "\n",
    "filenames = ['texts/emma.txt', 'texts/mansfield.txt', 'texts/northanger.txt',\n",
    "             'texts/persuasion.txt', 'texts/pride.txt', 'texts/sense.txt', 'texts/susan.txt']\n",
    "with open('texts/output.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = open(\"texts/output.txt\", \"r\").read().lower()\n",
    "\n",
    "# If you want to distinguish between uppercase and lowercase characters\n",
    "\n",
    "data = open(\"texts/output.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "# Sequence Length: the number of characters in a single training example\n",
    "\n",
    "seq_length = 30\n",
    "\n",
    "# Batch Size: \n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "# Number of neurons in the hidden layer\n",
    "\n",
    "hidden_layer = 128\n",
    "\n",
    "# Number of hidden layers\n",
    "\n",
    "no_layers = 1\n",
    "\n",
    "# Proportion of data set used for training\n",
    "\n",
    "train_prop = 0.95\n",
    "\n",
    "# Proportion of data set used for validation\n",
    "\n",
    "val_prop = 0.05\n",
    "\n",
    "# Inverse of dropout rate: each connection between layers will only be used with this probability\n",
    "\n",
    "keep_prob = 0.8\n",
    "\n",
    "\n",
    "# Split data into training set and validation set\n",
    "\n",
    "\n",
    "total_seqs = len(data) / seq_length\n",
    "\n",
    "train_seqs = int(train_prop * total_seqs)\n",
    "val_seqs = int(val_prop*total_seqs)\n",
    "\n",
    "train_data = data[:train_seqs*seq_length]\n",
    "val_data = data[:val_seqs*seq_length]\n",
    "\n",
    "train_data_len = len(train_data)\n",
    "val_data_len = len(val_data)\n",
    "\n",
    "# Resize data so that it is evenly divisible by \n",
    "\n",
    "data = data[:(train_seqs+val_seqs)*seq_length]\n",
    "\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "char_to_int = dict((ch, i) for i, ch in enumerate(chars))\n",
    "int_to_char = dict((i, ch) for i,ch in enumerate(chars))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "text_size = len(data)\n",
    "\n",
    "# Epoch size \n",
    "\n",
    "epoch_size = len(train_data) // (batch_size*seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode training and validation data to integers\n",
    "\n",
    "\"\"\"\n",
    "For training purposes, here we encode each character in the text as an integer value.\n",
    "\n",
    "e.g. 'a' is encoded to the value 60\n",
    "\"\"\"\n",
    "\n",
    "train_encoded = [char_to_int[char] for char in train_data]\n",
    "val_encoded = [char_to_int[char] for char in val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4173570  characters in the training set.\n",
      "There are 219660  characterists in the validation set.\n",
      "Each epoch consists of 6955 passes of length 600\n"
     ]
    }
   ],
   "source": [
    "# Some statistics\n",
    "\n",
    "print \"There are\", len(train_data), \" characters in the training set.\"\n",
    "print \"There are\", len(val_data), \" characterists in the validation set.\"\n",
    "print \"Each epoch consists of\", epoch_size, \"passes of length\", batch_size*seq_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "\n",
    "\"\"\" \n",
    "A placeholder is a variable to which we assign data at a future stage.\n",
    "It allows one to create operations and build computation graph without needing data.\n",
    "The computation graph is then \"fed\" data through these placeholders\n",
    "\"\"\"\n",
    "\n",
    "# Keep probability\n",
    "\n",
    "keep_prob_placeholder = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "\n",
    "# Batch-size\n",
    "\n",
    "batchsize = tf.placeholder(tf.int32, name = 'batchsize')\n",
    "\n",
    "# Inputs\n",
    "\n",
    "input_data = tf.placeholder(tf.uint8, [None, None], name = 'X')\n",
    "\n",
    "# Inputs one-hot encoded\n",
    "\n",
    "input_encoded = tf.one_hot(input_data, vocab_size, 1.0, 0.0)\n",
    "\n",
    "# Outputs\n",
    "\n",
    "output_data = tf.placeholder(tf.uint8, [None, None], name = 'Y_')\n",
    "output_encoded = tf.one_hot(output_data, vocab_size, 1.0, 0.0)\n",
    "\n",
    "# Input state\n",
    "\n",
    "hidden_state = tf.placeholder(tf.float32, [None, hidden_layer*no_layers], name = 'Hidden')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit\n",
    "\n",
    "cells = [tfc.rnn.GRUCell(hidden_layer) for _ in range(no_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive dropout\n",
    "\n",
    "dropcells = [tfc.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob_placeholder) for cell in cells]\n",
    "\n",
    "multicell = tfc.rnn.MultiRNNCell(dropcells, state_is_tuple=False)\n",
    "\n",
    "multicell = tfc.rnn.DropoutWrapper(multicell, output_keep_prob=keep_prob_placeholder) # dropout for softmax layer\n",
    "\n",
    "# Yr = outputs = tensor of shape [batch_size, seq_length, hidden_layer ] = [20, 30, 128]\n",
    "# H = hidden state = tensor of shape [batch_size, hidden_layer] = [20, 128]\n",
    "\n",
    "Yr, H = tf.nn.dynamic_rnn(cell, input_encoded, dtype=tf.float32, initial_state=hidden_state)\n",
    "\n",
    "# Softmax Layer implementation\n",
    "\n",
    "    # Flatten first two dimensions of output\n",
    "    \n",
    "W = tf.Variable(tf.random_normal([hidden_layer, vocab_size]))\n",
    "B = tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "# Reshape \n",
    "\n",
    "Yflat = tf.reshape(Yr, [-1, hidden_layer])\n",
    "Ylogits = tf.matmul(Yflat, W) + B\n",
    "\n",
    "Yflat_ = tf.reshape(output_encoded, [-1, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits = Ylogits, labels = Yflat_)\n",
    "loss = tf.reshape(loss, [batchsize, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Yo = tf.nn.softmax(Ylogits, name = 'Yo')\n",
    "Y = tf.argmax(Yo, 1)\n",
    "Y = tf.reshape(Y, [batchsize, -1], name = \"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent algorithm for minimizing loss function\n",
    "\n",
    "## ADAM - Adaptive Moment Estimation\n",
    "\n",
    "Like Adagrad, Adam computes adaptive learning rates for each parameter.\n",
    "It stores an exponentially decaying average of the past squared gradients - similar to RMSprop.\n",
    "Similar to momentum, Adam also retains an exponentially decaying past gradients.\n",
    "\n",
    "\n",
    "$ m_t$ = decaying average of past gradients  \n",
    "$ v_t$ = decaying average of past squared gradients\n",
    "\n",
    "$$m_t = \\beta_1m_{t-1} + (1-\\beta_1)g_t $$\n",
    "$$v_t = \\beta_2v_{t-1} + (1-\\beta_2)g^2_t $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats for Display\n",
    "\n",
    "seqloss = tf.reduce_mean(loss,1)\n",
    "batchloss = tf.reduce_mean(seqloss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(output_data, tf.cast(Y,tf.uint8)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Progress Bar\n",
    "\n",
    "display_freq = 50\n",
    "\n",
    "_50_batches  = display_freq * batch_size * seq_length\n",
    "\n",
    "progress = txt.Progress(display_freq, size = 111+2, msg = \"Training on next\" + str(display_freq) + \" batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise\n",
    "\n",
    "istate = np.zeros([batch_size, hidden_layer*no_layers]) # initial zero input state\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample from array of probabiltiies\n",
    "\n",
    "def sample_from_probabilities(probabilities, topn=vocab_size):\n",
    "    \"\"\"Roll the dice to produce a random integer in the [0..vocab_size] range,\n",
    "    according to the provided probabilities. If topn is specified, only the\n",
    "    topn highest probabilities are taken into account.\n",
    "    :param probabilities: a list of size vocab_size with individual probabilities\n",
    "    :param topn: the number of highest probabilities to consider. Defaults to all of them.\n",
    "    :return: a random integer\n",
    "    \"\"\"\n",
    "    p = np.squeeze(probabilities)\n",
    "    p[np.argsort(p)[:-topn]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(vocab_size, 1, p=p)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Random Text\n",
      "W&\n",
      "W\n",
      "U\n",
      "&2'&T0W3\n",
      "]�qUT0�ZT0\n",
      "&]\n",
      "&0]\n",
      "��\n",
      "&\n",
      "\n",
      "]\n",
      "\n",
      "��00]WU]WT0\n",
      "0&T0�\n",
      "\n",
      "]UU]g:TW��\n",
      ":�T�40U]T\n",
      "000�0�U0\n",
      "TTW]Z�\n",
      "ZZ&0\n",
      "\n",
      "\n",
      "T&T\n",
      "�\n",
      "]&&]\n",
      "&\n",
      "T\n",
      "]&]g\n",
      "W\n",
      "WB\n",
      "�]�0W��W:0�Z4&T]&\n",
      "&000T\n",
      "0\n",
      "TUT\n",
      "T]�]U\n",
      "&W&q40\n",
      "\n",
      "�T\n",
      "&W0\n",
      "&T0WvU0T]&]vW\n",
      "&&'��\n",
      "&&W\n",
      ",0]�TT\n",
      "Z&\n",
      "'gW\n",
      "\n",
      "Z&02TZ\n",
      "2W�\n",
      "q2�\n",
      "Z\n",
      "U&W\n",
      "&T&WW\n",
      "qqZ44J&00-W3\n",
      "]&3g3&WE\n",
      "&�&,0\n",
      "�T]\n",
      "Z&TT&,&\n",
      "W,�T]W�q\n",
      "0T0T0�]&W0W�\n",
      "&]\n",
      "���W0T0�U:TU\n",
      "�::U]U\n",
      "�:T]&0U]]U0\n",
      "]�TT��W�0]Ug0T]W]3UE��0T�WWT\n",
      "qf\n",
      "&\n",
      "T&,T]�WWW\n",
      "&qW\n",
      "q0\n",
      "]U�ZU\n",
      "Z&\n",
      "&'\n",
      "&W&'\n",
      "'�\n",
      "q\n",
      "\n",
      "q&'0K\n",
      "Z0]�T\n",
      "0TWUU&00�TT]WWWqq\n",
      "4&\n",
      "400&\n",
      "�\n",
      "&&'TU\n",
      "\n",
      "]��0]W\n",
      "&W\n",
      "&&\n",
      "0T\n",
      "&Wq\n",
      "�&\n",
      "\n",
      "\n",
      "&0W&qU\n",
      "W\n",
      "&\n",
      "�0&W\n",
      "T�U]\n",
      "W0\n",
      "Wq�&TU�WU�]WT&Wq\n",
      "&�ZU0\n",
      "\n",
      "T&0W&],\n",
      "0&TT0&00U\n",
      "0T0TT0&\n",
      "\n",
      "��]\n",
      "&&\n",
      "\n",
      "�f&W�\n",
      "&,\n",
      "UUT]W0\n",
      "\n",
      "T]T]WW��T&�00\n",
      "TW\n",
      "T]&0]Z0gg0\n",
      "0\n",
      "&T'00\n",
      "��U0\n",
      "T�T0]]�U�W0Z�&\n",
      "W�\n",
      "00UW&\n",
      "\n",
      "]\n",
      "�\n",
      "]&W&q\n",
      "&�U\n",
      "&\n",
      "&&W00\n",
      "$,W0\n",
      "\n",
      "[T&\n",
      "0�\n",
      "&]\n",
      "U&&T&WW&0\n",
      "00\n",
      "T&WU]v'0]WW3T3&W�q\n",
      "\n",
      "&U]00T0\n",
      "\n",
      "&0\n",
      "U&\n",
      "�0\n",
      "\n",
      "T&\n",
      "\n",
      "\n",
      "f&UTUU]]\n",
      "\n",
      "]�Z�\n",
      "\n",
      "U]&0]\n",
      "�U\n",
      "U]\n",
      "n00W&\n",
      "UW\n",
      "qq&T\n",
      "]qWq2Z\n",
      "22&\n",
      "�0\n",
      "U�&\n",
      "0\n",
      "�0WT]\n",
      "��]WZgUg\n",
      "]U0W0\n",
      "\n",
      "U��TTTT0\"\n",
      "W&W\n",
      "&\n",
      "0W&U\n",
      "\n",
      "]&0,\n",
      "\n",
      "0&T0]W3�T&\n",
      "]W�U\n",
      "U:]�Z0U3UTg0W:00\n",
      "c0U\n",
      "\n",
      "cWTWU�T2\n",
      "&�WU]3TU]n\n",
      "&Ug0g]300\n",
      "\n",
      "\n",
      "&�\n",
      "&T0]\n",
      "]T�0�]�0�ZZ4gU\n",
      "Ug]0TT\n",
      "\n",
      "&\n",
      "T&]W\n",
      "&WU\n",
      "nW&q2&q0\n",
      "&T&W\n",
      "00T&0TT&0T\n",
      "TEXT-GEN DONE\n",
      "\n",
      "()\n",
      "0%                                        Training on next50 batches                                         100%\n",
      "=================================================================================================================()\n",
      "0%                                        Training on next50 batches                                         100%\n",
      "=================================================================================================================()\n",
      "0%                                        Training on next50 batches                                         100%\n",
      "================================================="
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-abbdbf0bb643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                  keep_prob_placeholder: keep_prob, batchsize: batch_size}\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mostate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Display a short text generated with current weights and biases (every 150 batches)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fhl/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fhl/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fhl/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/fhl/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fhl/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "for x, y_, epoch in txt.rnn_minibatch_sequencer(train_encoded, batch_size, seq_length, nb_epochs=2):\n",
    "    \n",
    "    # train on one minibatch\n",
    "    \n",
    "    feed_dict = {input_data:x,\n",
    "                 output_data:y_,\n",
    "                 hidden_state: istate,\n",
    "                 keep_prob_placeholder: keep_prob,\n",
    "                 batchsize: batch_size}\n",
    "    \n",
    "    _, y, ostate = sess.run([train_step, Y, H], feed_dict = feed_dict)\n",
    "    \n",
    "    # Display a short text generated with current weights and biases (every 150 batches)\n",
    "    \n",
    "    if step // 3 % _50_batches == 0:\n",
    "        \n",
    "        print\n",
    "        print \"Generating Random Text\"\n",
    "    \n",
    "        ry = np.array([[char_to_int['k']]])\n",
    "        rh = np.zeros([1, hidden_layer*no_layers])\n",
    "    \n",
    "        for k in range(1000):\n",
    "\n",
    "            ryo, rh = sess.run([Yo, H], feed_dict = {input_data: ry,\n",
    "                                                     keep_prob_placeholder: 1.0,\n",
    "                                                     hidden_state: rh,\n",
    "                                                     batchsize: 1})\n",
    "            \n",
    "            rc = sample_from_probabilities(ryo, topn = 10 if epoch <= 1 else 2)\n",
    "            \n",
    "            sys.stdout.write(int_to_char[rc])\n",
    "            \n",
    "            ry = np.array([[rc]])\n",
    "        \n",
    "        print\n",
    "        print \"TEXT-GEN DONE\"\n",
    "        print\n",
    "    \n",
    "    # display progress bar\n",
    "    \n",
    "    progress.step(reset = step % _50_batches == 0)\n",
    "    \n",
    "    # loop state around\n",
    "    \n",
    "    istate = ostate\n",
    "    step += batch_size * seq_length\n",
    "    \n",
    "print\n",
    "print \"---Finished Training---\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
